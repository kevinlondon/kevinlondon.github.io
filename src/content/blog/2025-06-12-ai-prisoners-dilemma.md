---
title: "The AI Prisoner's Dilemma"
description: ""
isDraft: true
pubDatetime: 2025-06-08T22:00:00-8:00
tags:
- gamedev
- ai
- cursor
- meta
---

I've been thinking about this thing I'm calling the AI prisoner's dilemma. Every
few months, the tech industry swings between breathless AI evangelism and dire
warnings about technological apocalypse. Beneath all the hot takes, there's this
weird structural problem: we might be racing toward something none of us want.

Here's the dilemma. If we all abstain from using AI, we preserve the status quo
in the short term, and there's a risk of being blind-sided by the innovator's
dilemma. Those who adopt AI could gain a competitive advantage so significant
that those who abstain become less relevant.

If we all embrace AI, we benefit from a brief spike in productivity, and we
accelerate towards a possible future where, as [Dario Amodei recently
warned](https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic),
half of all entry-level white-collar jobs could disappear, potentially
increasing unemployment to up to 10-20%. 

The reality is there - by using the tools that are built to replace our craft,
what happens to the craft?

The trap is that individual rational behavior (adopting AI to stay competitive)
leads to collectively irrational outcomes (mass technological unemployment).
Unlike traditional prisoner's dilemmas, we can't easily coordinate our way out
of this one. 

## The AI Ratchet

The progression feels inescapable once you're on it. A year ago, I wrote 90-100%
of my code by hand. Then GitHub Copilot arrived in my life, and suddenly I wrote
70-80%, while the AI handled boilerplate and common patterns. Six months later,
with VSCode and agent selection, I was down to 50-60% human contribution. Today,
I'm writing maybe 20-30% of my code directly. I'm still valuable as an editor
for AI (I think!), and my role in the relationship has changed from musician to
conductor.

Each step felt like a reasonable optimization. Why write repetitive code when AI
can do it faster and better? The cumulative effect is significant: I can see a
near future—perhaps 1-2 years away where I'm not writing code in an editor at
all, and rather managing teams of AI agents that implement what I describe.

This trajectory isn't unique to me. The major tools are all moving toward an
agent orchestration model: [OpenAI's
Codex](https://openai.com/index/introducing-codex/), [Claude Code with a Claude
Code MCP](https://docs.anthropic.com/en/docs/claude-code/mcp), [Cursor's
BugBot](https://docs.cursor.com/bugbot). The writing is on the wall, and it's
written in code I didn't write.

## The Sports Gear Analogy—And Why It Breaks Down

A friend recently compared AI adoption to how sports equipment has evolved: "Do
you really want to play baseball in 1930s gear? Or basketball in what they wore
in the 70s?" He pointed out that ultramarathons have had to be made more
difficult over time because gear improvements made the original courses too
easy.

The analogy is compelling, and I think it misses the magnitude of change we're
experiencing. This isn't about incrementally better equipment, and it's about
category shifts happening at breakneck speed.

Imagine if cycling went from Tour de France bicycles to e-bikes with
race-lasting batteries, then to motorcycles, all within five years. At some
point, you're not playing the same sport anymore, and you're not even the same
type of athlete.

## Magic Hour

There's a moment in photography called magic hour. That brief period after
sunset when the sky is still light and night is approaching. The light is
beautiful, and you know it's temporary.

I think that's where we are with AI and knowledge work. We're in this golden
hour where AI amplifies our capabilities rather than replacing them. I'm more
productive than I've ever been. The work is often more interesting because the
tedious parts are automated away. Night is coming, and I genuinely don't know
what's on the other side of it.

## The Illusion of Agency

Which brings us to the central question: Do we actually have meaningful choice
in how this plays out?

Individual developers, companies, even entire industries are making rational
decisions to adopt AI tools. These micro-decisions aggregate into macro-outcomes
that no single entity controls or fully intends. It's the challenge of
collective action distributed over an industry. 

The prisoner's dilemma framework suggests our options are more limited than we'd
like to believe. Even if we could coordinate globally to slow AI adoption, the
incentives for breaking rank and using AI would be enormous. The first movers
gain such advantages that abstaining becomes irrational.

## Three Potential Futures

I'm not sure where this all goes, and I'm not great at predictions! That said,
here are a few possible futures I could see:

**Soft Landing:** AI becomes a powerful collaborative tool and doesn't fully
*replace human cognitive work. We find a new equilibrium where humans and AI
*work together, similar to how spreadsheets made certain calculations trivial
*and created new types of analysis work.

**Hard Transition:** Displacement happens quickly, and economic and social
*systems adapt. Universal basic income, shorter work weeks, or new economic
*models emerge. History suggests humans are adaptable to technological change,
*even when it's initially disruptive.

**Overshoot:** We automate faster than we can adapt. Social and economic systems
*strain under the change. The benefits of AI are captured by a small number of
*entities while the effects ripple through society faster than safety nets can
*be built.

## Living in the Question

The honest answer is that I don't know which future we're heading toward, and
I'm not sure how much individual choices can impact the direction. The AI
prisoner's dilemma suggests we're all making locally rational decisions that
push us toward broadly uncertain results.

Recognizing the dilemma doesn't mean we're powerless. Understanding the forces
might help us make better individual choices and better collective ones. If
we're going to race toward AI adoption, we might as well race toward versions
that serve human flourishing rather than efficiency metrics.

I don't have answers about what comes next. What I do know is that the choices
we make now—whether to embrace AI, resist it, or just try to keep up—are shaping
the future in ways we can't really see yet. Maybe that's not such a bad thing.
Some of the best innovations have come from periods of uncertainty, when we had
to figure things out as we went along.

Magic hour doesn't last forever, and neither does the uncertainty. Eventually,
we'll find our footing in whatever comes next. 
Night can be beautiful too, and I think we'll find our light.