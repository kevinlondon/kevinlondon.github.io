---
title: "The AI Prisoner's Dilemma"
description: ""
isDraft: true
pubDatetime: 2025-06-08T22:00:00-8:00
tags:
- gamedev
- ai
- cursor
- meta
---

It feels like we're stuck in a strange paradox in the tech industry.
Simultaneously, there's AI evangelism and exuberance, those predicted dire
outcomes about a technological apocalypse, and people who think basically
nothing will change and it's all overhyped. Beneath it, it feels like we're
being forced into this Faustian bargain where we have to make personal choices
about how much AI to interact with, and where. Where we draw our own lines.

Option A: Abstain from using AI. Preserved the status quo, but there's a risk of
being blind-sided by a different dilemma, the innovator's dilemma. Those who
adopt AI and embrace it could gain a competitive advantage so significant that
those who abstain become less relevant.

Option B: Embrace AI. We benefit from a brief spike in productivity but with
some drawbacks! In this approach, we accelerate towards a possible future where,
as [Dario Amodei recently
warned](https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic),
half of all entry-level white-collar jobs could disappear, potentially
increasing unemployment to up to 10-20%. 

Granted, news headlines are by their nature attention-grabbing. But the reality
is there - by using the tools that are built to replace our craft, .... what
happens to the craft?

The trap is that individual rational behavior (adopting AI to stay competitive)
leads to collectively irrational outcomes (mass technological unemployment). And
unlike traditional prisoner's dilemmas, we can't easily coordinate our way out
of this one. 

## Ratching AI

The progression feels inescapable once you're on it. A year ago, I wrote 90-100% of
my code by hand. Then GitHub Copilot arrived in my life, and suddenly I wrote 70-80%, while
the AI handled boilerplate and common patterns. Six months later, with VSCode
and agent selection, I was down to 50-60% human contribution. Today, I'm writing
maybe 20-30% of my code directly. I'm still valuable as an editor for AI (I think!), but my
role in the relationship has changed from musician to conductor.

Each step felt like a reasonable optimization. Why write repetitive code when AI
can do it faster and, increasingly, better? But the cumulative effect is significant: I can
see a near future—perhaps 1-2 years away where I'm not writing code in an editor
at all, but rather managing teams of AI agents that implement what I describe.

This trajectory isn't unique to me. The major tools are all moving toward an
agent orchestration model: [OpenAI's Codex](https://openai.com/index/introducing-codex/), [Claude Code with a Claude Code MCP](https://docs.anthropic.com/en/docs/claude-code/mcp), [Cursor's
BugBot](https://docs.cursor.com/bugbot). The writing is on the wall, and it's written in code I didn't write.

## The Sports Gear Analogy—And Why It Breaks Down

A friend recently compared AI adoption to how sports equipment has evolved: "Do
you really want to play baseball in 1930s gear? Or basketball in what they wore
in the 70s?" He pointed out that ultramarathons have had to be made more
difficult over time specifically because gear improvements made the original
courses too easy.

The analogy is compelling, and I think it misses the magnitude of change we're
experiencing. This isn't about incrementally better equipment, it's about
category shifts happening at hard-to-image speed.

Imagine if cycling went from Tour de France bicycles to e-bikes with
race-lasting batteries, then to motorcycles, all within five years. At some
point, you're not playing the same sport anymore. And you're not even the same type
of athlete.

## Magic Hour

There's a moment in photography called magic hour. That brief period after sunset
when the sky is still light and night is approaching. The light is beautiful,
 but you know it's temporary.

I think that's where we are with AI and knowledge work. We're in this golden
hour where AI amplifies our capabilities rather than replacing them. I'm more
productive than I've ever been, or at least I hope. The work is often more
interesting because the tedious parts are automated away. But night is coming,
and I genuinely don't know what's on the other side of it.

## The Illusion of Agency

Which brings us to the central question: Do we actually have meaningful choice
in how this plays out?

Individual developers, companies, even entire industries are making rational
decisions to adopt AI tools. But these micro-decisions aggregate into
macro-outcomes that no single entity controls or fully intends. It's the challenge of collective action distributed over an industry. 

The prisoner's dilemma framework suggests our options are more limited than we'd
like to believe. Even if we could somehow coordinate globally to slow AI
adoption, the incentives for breaking rank and using AI would be enormous. 
The first movers gain such advantages that abstaining becomes irrational.

## Three Potential Futures

I'm not sure where this all goes, and I'm not great at predictions! That said, here's a few
possible futures I could see:

**Soft Landing:** AI becomes a powerful collaborative tool but doesn't fully
replace human cognitive work. We find new a place where humans and AI work
together, similar to how spreadsheets made certain calculations trivial but
created new types of analysis work.

**Hard Transition:** Displacement happens quickly, but economic and
social systems adapt. Universal basic income, shorter work weeks, or 
new economic models emerge. History suggests humans are adaptable to
technological change, even when it's initially disruptive.

**Overshoot:** We automate faster than we can adapt. Social and economic
systems strain under the change. The benefits of AI are captured by a small
number of entities while the effects ripple through society faster
than safety nets can be built.

## Living in the Question

The honest answer is that I don't know which future we're heading toward, and
I'm not sure how much individual choices can impact the direction. 
The AI prisoner's dilemma
suggests we're all making locally rational decisions that push us toward
broadly uncertain results.

But recognizing the dilemma doesn't mean we're powerless. Understanding the
forces might help us make better individual choices and, more
importantly, better collective ones. If we're going to race toward AI adoption
anyway, we might as well race toward versions that serve humankind 
rather than just efficiency metrics.

Magic hour doesn't last forever. But sometimes, if you're paying attention, you
can catch glimpses of what's coming in the quality of the light itself.