---
title: "The AI Prisoner's Dilemma"
description: "Individual logical choices about AI adoption might be leading us toward outcomes none of us actually want. Here's why we might be trapped in a collective dilemma."
pubDatetime: 2025-06-12T20:44:00-8:00
tags:
- ai
- technology
- productivity
- future
- meta
---

I've been thinking about this thing I'm calling the AI prisoner's dilemma. The
tech industry can't make up its mind about AI: one month AI's going to save us
all, the next it's going to destroy everything. Beneath all the hot takes,
there's a weird structural problem.

In the classic [prisoner's dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma), individual rational choices lead to outcomes that are worse for everyone than if they'd just cooperated.

If we _avoid_ using AI, we might miss out on productivity gains and fall behind 
those who do adopt it. If we _embrace_ AI, we get those benefits, but we might 
also be accelerating toward outcomes we can't fully predict. Maybe it's a future 
where many jobs change significantly, as [Dario Amodei recently suggested](https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic).
Maybe it's something we haven't even considered yet.

The question that keeps nagging at me: by using tools built to replace our
craft, what happens to the craft itself?

What makes this tricky is that individual rational choices (using AI to stay 
competitive) might lead to collective outcomes that none of us actually want. 
Unlike other prisoner's dilemmas, we can't easily coordinate to avoid this. 

## The AI Ratchet

Once you're on the AI path, it feels inevitable. Two years ago, I wrote 90-100%
of my code by hand. Then GitHub Copilot arrived in my life, and suddenly I wrote
maybe 70-80%, while AI handled boilerplate and common patterns. A year later,
with VSCode and agent selection, I was down to 50-60% human contribution. Today,
I'm writing maybe 30% of my code directly. I know this sounds hard to believe! I 
wrote about [building a game with AI recently](/2025/06/04/building-game-with-ai/) 
where I barely wrote any code. I'm still valuable as an editor
for AI, and my role in the relationship is changing from musician to
conductor.

Each step felt like a reasonable optimization. Why write repetitive code when AI
can do it faster and better? The cumulative effect is significant: I can see a
near future (perhaps 1-2 years away) where I'm not writing code in an editor at
all, and rather managing teams of AI agents that implement what I describe.

This isn't unique to me. The major tools are all moving toward an
agent orchestration model: [OpenAI's
Codex](https://openai.com/index/introducing-codex/), [Claude Code with a Claude
Code MCP](https://docs.anthropic.com/en/docs/claude-code/mcp), [Cursor's
Background Agents](https://docs.cursor.com/background-agent). The writing is on the wall, and it's
written in code I didn't write.

## The Sports Gear Analogyâ€”And Why It Breaks Down

A friend recently compared AI adoption to sports equipment evolution: "Do you
really want to play baseball in 1930s gear? Or basketball in what they wore in
the 70s?" He pointed out that ultramarathons had to be made harder over time
because gear improvements made the original courses too easy.

I get the analogy, but I think it misses how big this change feels. This isn't
about incrementally better equipment, it's about category shifts happening at
a rapid pace.

Imagine if cycling went from Tour de France bicycles to e-bikes with
race-lasting batteries, then to motorcycles, all within five years. At some
point, you're not playing the same sport anymore, and you're not even the same
type of athlete.

## Magic Hour

There's a moment in photography called magic hour. That brief period after
sunset when the sky is still light. It's beautiful and very short-lived.

![Magic hour at Pike Place Market](/assets/ai-prisoners/2020-06-02.jpg)
_Magic hour at Pike Place Market_

I think that's where we are with AI and knowledge work. We're in this golden
hour where AI multiplies our capabilities rather than replacing them. I'm more
productive than I've ever been. The work is often more interesting because the
tedious parts are automated away. Something new is coming, and I genuinely don't know
what's on the other side of it.

## The Illusion of Agency

Which brings us to the central question: Do we actually have meaningful choice
in how this plays out?

Individual developers, companies, even entire industries keep choosing to adopt
AI tools. Each choice seems reasonable on its own. Yet these small decisions
add up to big outcomes that no one controls or really wants. Coordinating
across a whole industry feels nearly impossible. 

I suspect our options are more limited than we'd like to believe. Even if we 
could somehow coordinate to slow AI adoption, the incentives for breaking rank 
would be huge. The people who adopt AI first may get such significant advantages that 
staying out starts to feel impossible.

## What We Can and Can't Control

When I feel overwhelmed by all this, I try to focus on what's actually in my control 
versus what isn't. 

**Out of my control:** Industry direction, macro economic changes, whether AI development 
slows down or speeds up, what other companies or developers choose to do.

**In my control:** My curiosity about these tools, willingness to learn how to use 
them effectively, staying informed, and how I adapt to changes.

I can't control whether we're heading toward a soft landing or something more disruptive. 
But I can control whether I'm learning and staying curious rather than just reacting 
with fear or avoidance.

## Three Potential Futures

I'm not sure where this all goes, and I'm not great at predictions! That said,
here are a few possible futures I could see:

**Soft Landing:** AI becomes a powerful collaborative tool and doesn't fully
replace human cognitive work. We find a new state where humans and AI
work together, similar to how spreadsheets made certain calculations trivial
and created new types of analysis work.

**Hard Transition:** Displacement happens quickly, but we figure it out. Maybe
universal basic income, shorter work weeks, or new economic models emerge. 
Humans have adapted to big technological changes before, even when they felt 
overwhelming at the time.

**Overshoot:** We automate faster than we can adapt. Social and economic systems
strain under the change. The benefits of AI are captured by a small number of
entities while the effects ripple through society faster than safety nets can
be built.

## Living in the Question

The honest answer is that I don't know which future we're heading toward, and
I'm not sure how much individual choices can impact the direction. The AI
prisoner's dilemma suggests we're all making locally rational decisions that
push us toward uncertain results.

Recognizing the dilemma doesn't mean we're powerless! Understanding the forces
might help us make better individual choices and better collective ones. If
we're going to race toward AI adoption, we might as well do it intentionally.

I don't have answers about what comes next. What I do know is that the choices
we make now are shaping the future in ways we can't really see yet. Maybe that's not such a bad thing.
Some of the best innovations have come from periods of uncertainty, when we had
to figure things out as we went along.

Magic hour doesn't last forever, and neither does the uncertainty. Eventually,
we'll find our footing in whatever comes next. Dawn can be beautiful too.